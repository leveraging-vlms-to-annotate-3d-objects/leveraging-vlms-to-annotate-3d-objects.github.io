<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <title>Leveraging VLM-Based Pipelines to Annotate 3D Objects</title>
    <meta content="Leveraging VLM-Based Pipelines to Annotate 3D Objects [ICML 2024]" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script> 
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SGF85932PS"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-SGF85932PS');
    </script>
    <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> -->
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>

    <style>
        table {
          width: 100%;
          border-collapse: collapse;
        }
    
        table, th, td {
          border: 1px solid black;
        }
    
        th, td {
          padding: 8px;
          text-align: center;
        }
    
        th {
          background-color: #f2f2f2;
        }
        .containerside {
            width: 45%;
            float: left;
            margin: 2.5%;
        }

        .image-container1 {
        width: 40%; /* Set the width to 80% of the parent container */
        max-width: 400px; /* Set a maximum width to prevent excessive scaling */
        margin-left: auto;
        margin-right: auto;
    }

    .image-container1 img {
        width: 100%; /* Make the image fill its container */
        height: auto; /* Maintain the aspect ratio */
    }

    .image-container2 {
        width: 65%; /* Set the width to 80% of the parent container */
        max-width: 700px; /* Set a maximum width to prevent excessive scaling */
        margin-left: auto;
        margin-right: auto;
    }

    .image-container2 img {
        width: 100%; /* Make the image fill its container */
        height: auto; /* Maintain the aspect ratio */
    }

        .image-container3 {
        width: 80%; /* Set the width to 80% of the parent container */
        max-width: 900px; /* Set a maximum width to prevent excessive scaling */
        margin-left: auto;
        margin-right: auto;
    }

    .image-container3 img {
        width: 100%; /* Make the image fill its container */
        height: auto; /* Maintain the aspect ratio */
    }

    .wtable {
        width: 90%;
        max-width: 1000px;
        margin-left: auto;
        margin-right: auto;
    }

    /* Media query for smaller screens */
    @media (max-width: 600px) {
        .wtable {
            width: 100%;
        }

        /* Increase specificity for smaller screens */
        .wtable td {
            font-size: 7px !important; 
            max-width: 30px !important;
            white-space: normal;
        }
        .wtable th {
            font-size: 7px !important; 
            max-width: 30px !important;
            white-space: normal;
        }
    }

    /* Common styles for both large and small screens */
    .wtable table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 15px;
        margin-left: auto;
        margin-right: auto;
    }

    .wtable th, .wtable td {
        border: 1px solid #ddd;
        padding: 8px;
    }

    .wtable th {
        background-color: #f2f2f2;
    }

    @media (max-width: 600px) {
    .white_section_nerf_ref {
        display: none; /* Hide the section on smaller screens */
    }
    }   
    

    </style>

</head>


<body>
    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">Leveraging VLM-Based Pipelines</h1>
            <h1 class="nerf_title_v2">to Annotate 3D Objects</h1>
            <!-- <div class="nerf_subheader_v2" style="color: #ff7411;"><b>ICML 2024</b></div> -->
            <div class="nerf_subheader_v2"><b>[ICML 2024]</b></div>
        </div>
            <div class="nerf_subheader_v2">
                <div>
                    <a href="https://www.rkabra.com/" target="_blank" class="nerf_authors_v2">Rishabh Kabra<span
                            class="text-span_nerf"></span></a><sup> 1, 2</sup>,&nbsp;&nbsp;
                    <a href="https://matthey.me/" target="_blank" class="nerf_authors_v2">Loic Matthey<span
                            class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=wBxAt8cAAAAJ" target="_blank" class="nerf_authors_v2">Alexander Lerchner<span
                            class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;
                    <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/" target="_blank" class="nerf_authors_v2">Niloy J. Mitra<span
                            class="text-span_nerf"></span></a><sup> 2</sup>
                </div>
                <div>
                    <h1 class="nerf_affiliation_v2"><sup>1</sup> Google DeepMind</h1>,
                    <h1 class="nerf_affiliation_v2"><sup>2</sup> University College London</h1>
                </div>

                <div class="external-link">
                    <a class="btn" href="https://arxiv.org/abs/2311.17851" role="button" target="_blank">
                        <i class="fa fa-file-pdf"></i> Paper </a>
                    <a class="btn" href="https://github.com/google-deepmind/objaverse_annotations" role="button" target="_blank">
                        <i class="fa-brands fa-github"></i> Dataset </a>
                </div>

            </div>
        <div class="image-container2">
            <img src="assets/teaser.png">
        </div>
    </div>
    <br>


    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf">Abstract</h2>
            <p class="paragraph-3 nerf_text nerf_results_text">
    		Pretrained vision language models (VLMs) present an opportunity to caption unlabeled 3D objects at scale. The leading approach to summarize VLM descriptions from different views of an object (Luo et al., 2023) relies on a language model (GPT4) to produce the final output. 
            This text-based aggregation is susceptible to hallucinations as it merges potentially contradictory descriptions.
            We propose an alternative algorithm to marginalize over factors such as the viewpoint which affect the VLM's response. Instead of merging text responses, we utilize the VLM's joint image-text likelihoods.
            We show our probabilistic aggregation is not only more reliable and efficient, but sets the SoTA on inferring object types with respect to human-verified labels.
            The aggregated annotations are also useful for conditional inference; they improve downstream predictions (e.g., of object material) when the objectâ€™s type is specified as an auxiliary text-based input.
            Such auxiliary inputs allow ablating the contribution of visual reasoning over visionless reasoning in an unsupervised setting.
            With these supervised and unsupervised evaluations, we show how a VLM-based pipeline can be leveraged to produce reliable annotations for 764K objects from the Objaverse dataset.
            <br>
            </p>
        </div>
    </div>

    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf">1. Language models hallucinate</h2>
        <!-- <h2 class="grey-heading_nerf">  even when summarizing descriptions of the same object</h2> -->
        <p class="paragraph-3 nerf_text nerf_results_text"> Prior work that relies on GPT-4 to aggregate multi-view object descriptions is prone to contain hallucinated details:</p>
        <div class="grid-container-2">
            <div class="grid-item">  
                <video height="100%" autoplay muted controls>
                    <source src="assets/cap3d_0f352994f8c141f091779ab91710c3f6.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
            <div class="grid-item">
                <video  height="100%" autoplay muted controls>
                    <source src="assets/cap3d_73668b6cc68e412dabc3147cd4aa9b2d.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
            <div class="grid-item">
                <video  height="100%" autoplay muted controls>
                    <source src="assets/cap3d_0d44f84ec5824261837ab03694b5a8d5.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
            <div class="grid-item">
                <video  height="100%" autoplay muted controls>
                    <source src="assets/cap3d_7420268e830c4c7d9c04894d06360b25.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
        </div>
    </div>

    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf">2. A simple way to mitigate hallucinations</h2>
        <p class="paragraph-3 nerf_text nerf_results_text">We propose an algorithm, <em>ScoreAgg</em>, which relies on visually grounded VLM scores to produce reliable summaries. These scores can be derived for free while sampling from the VLM. ScoreAgg requires no additional modules, nor any model retraining. It is a simple way to boost the accuracy of zero-shot VLM inference.</p>
    </div>

    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf">Quantitative Comparison</h2>
        <p class="paragraph-3 nerf_text nerf_results_text">See Figure 3 in the paper for a quantitative comparison.</p>
    </div>
    
    <br>
    
    <div class="white_section_nerf_ref w-container">
        <h2 class="grey-heading_nerf">References</h2>
        <!-- <br> -->
        <p class="paragraph-3 nerf_text nerf_results_text">[1] Luo, Tiange, et al. "Scalable 3d captioning with pretrained models." Advances in Neural Information Processing Systems 36 (2023).</p>
       <br><br><br>
        <!-- <pre><code>

      </code></pre> -->
    </div>


    <div class="white_section_nerf grey_container w-container">
        <h2 class="grey-heading_nerf">BibTeX</h2>
        <div class="bibtex">
            <pre><code>
@inproceedings{ICML2024_LeveragingVLMs,
 author = {Kabra, Rishabh and Matthey, Loic and Lerchner, Alexander and Mitra, Niloy J.},
 title = {Leveraging VLM-Based Pipelines to Annotate 3D Objects},
 booktitle = {Proceedings of the 41st International Conference on Machine Learning},
 series = {Proceedings of Machine Learning Research},
 publisher = {PMLR},
 volume = {235},
 year = {2024}
}          
            </code></pre>
        </div>
    </div>

</body>

<footer>
    Template adapted from <a href="https://mrtornado24.github.io/DreamCraft3D/" target="_blank">DreamCraft3D</a> <br>
    <!-- <div style="width: 60px; height: 20px; display: flex; align-items: center; justify-content: center;">
        <a href="https://hits.seeyoufarm.com">
            <img style="width: 100%;" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fdiff3f.github.io&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false" alt="hits"/>
        </a>
    </div> -->
    
    <br><br><br>
</footer> 
</html>
