<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <title>Leveraging VLM-Based Pipelines to Annotate 3D Objects</title>
    <meta content="Leveraging VLM-Based Pipelines to Annotate 3D Objects [ICML 2024]" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script> 
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SGF85932PS"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-SGF85932PS');
    </script>
    <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> -->
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>
    <style>
        table {
          width: 100%;
          border-collapse: collapse;
        }
    
        table, th, td {
          border: 1px solid black;
        }
    
        th, td {
          padding: 8px;
          text-align: center;
        }
    
        th {
          background-color: #f2f2f2;
        }
        .containerside {
            width: 45%;
            float: left;
            margin: 2.5%;
        }

        .image-container1 {
        width: 40%; /* Set the width to 80% of the parent container */
        max-width: 400px; /* Set a maximum width to prevent excessive scaling */
        margin-left: auto;
        margin-right: auto;
    }

    .image-container1 img {
        width: 100%; /* Make the image fill its container */
        height: auto; /* Maintain the aspect ratio */
    }

    .image-container2 {
        width: 65%; /* Set the width to 80% of the parent container */
        max-width: 700px; /* Set a maximum width to prevent excessive scaling */
        margin-left: auto;
        margin-right: auto;
    }

    .image-container2 img {
        width: 100%; /* Make the image fill its container */
        height: auto; /* Maintain the aspect ratio */
    }

        .image-container3 {
        width: 80%; /* Set the width to 80% of the parent container */
        max-width: 900px; /* Set a maximum width to prevent excessive scaling */
        margin-left: auto;
        margin-right: auto;
    }

    .image-container3 img {
        width: 100%; /* Make the image fill its container */
        height: auto; /* Maintain the aspect ratio */
    }

    .wtable {
        width: 90%;
        max-width: 1000px;
        margin-left: auto;
        margin-right: auto;
    }

    /* Media query for smaller screens */
    @media (max-width: 600px) {
        .wtable {
            width: 100%;
        }

        /* Increase specificity for smaller screens */
        .wtable td {
            font-size: 7px !important; 
            max-width: 30px !important;
            white-space: normal;
        }
        .wtable th {
            font-size: 7px !important; 
            max-width: 30px !important;
            white-space: normal;
        }
    }

    /* Common styles for both large and small screens */
    .wtable table {
        width: 100%;
        border-collapse: collapse;
        margin-top: 15px;
        margin-left: auto;
        margin-right: auto;
    }

    .wtable th, .wtable td {
        border: 1px solid #ddd;
        padding: 8px;
    }

    .wtable th {
        background-color: #f2f2f2;
    }

    @media (max-width: 600px) {
    .white_section_nerf_ref {
        display: none; /* Hide the section on smaller screens */
    }
    }   
    

    </style>

</head>


<body>
    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">Leveraging VLM-Based Pipelines to Annotate 3D Objects</h1>
            <div class="nerf_subheader_v2">Leveraging VLM-Based Pipelines to Annotate 3D Objects</div>
            <!-- <div class="nerf_subheader_v2" style="color: #ff7411;"><b>ICML 2024</b></div> -->
            <div class="nerf_subheader_v2"><b>[ICML 2024]</b></div>
        </div>
            <div class="nerf_subheader_v2">
                <div>
                    <a href="https://rkabra.com/" target="_blank" class="nerf_authors_v2">Rishabh Kabra<span
                            class="text-span_nerf"></span></a><sup> 1, 2</sup>,&nbsp;&nbsp;
                    <a href="https://https://matthey.me/" target="_blank" class="nerf_authors_v2">Loic Matthey<span
                            class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=wBxAt8cAAAAJ" target="_blank" class="nerf_authors_v2">Alexander Lerchner<span
                            class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;
                    <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/" target="_blank" class="nerf_authors_v2">Niloy J. Mitra<span
                            class="text-span_nerf"></span></a><sup> 2</sup>
                </div>
                <div>
                    <h1 class="nerf_affiliation_v2"><sup>1</sup> Google DeepMind</h1>,
                    <h1 class="nerf_affiliation_v2"><sup>2</sup> University College London</h1>
                </div>

                <div class="external-link">
                    <a class="btn" href="https://arxiv.org/abs/2311.17024" role="button" target="_blank">
                        <i class="ai ai-arxiv"></i> Arxiv </a>
                    <a class="btn" href="https://arxiv.org/pdf/2311.17024.pdf" role="button" target="_blank">
                        <i class="fa fa-file-pdf"></i> Paper </a>
                    <a class="btn" href="https://github.com/niladridutt/Diffusion-3D-Features" role="button" target="_blank">
                        <i class="fa-brands fa-github"></i> Code </a>
                    <!-- <a class="btn btn-large btn-light" href="https://youtu.be/0FazXENkQms" role="button" target="_blank" disabled>
                        <i class="fa-brands fa-youtube"></i> Video </a> -->
                </div>

            </div>
        <div class="image-container2">
            <img src="assets/teaser.jpg">
        </div>
        <div class="container-2 nerf_header_v2 w-container" style="text-align: left;">
        <p>Diff3F is a a novel feature distiller that harnesses the expressive power of in-painting diffusion features and distills them to points on 3D surfaces. Here, the proposed features are employed for point-to-point shape correspondence between assets varying in shape, pose, species and topology. We achieve this without any fine-tuning of the underlying diffusion models, and demonstrate results on untextured meshes, point clouds, and raw scans. Note that we show raw point-to-point correspondence, without any regularization or smoothing. Inputs are <b>point clouds, non-manifold meshes, or 2-manifold meshes</b>. The left most mesh is the source and all remaining 3D shapes are targets. Corresponding points are similarly colored.</p>
        </div>
    </div>
    <br>


    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf">Abstract</h2>
 url = {https://openreview.net/forum?id=5Pcl5qOOfL&noteId=hqA5vzICE9},
            <p class="paragraph-3 nerf_text nerf_results_text">
                We present Diff3F as a simple, robust, and class-agnostic feature descriptor that can be computed for untextured input shapes (meshes or point clouds). 
                Our method distills diffusion features from image foundational models onto input shapes. Specifically, we use the input shapes to produce depth and normal maps 
                as guidance for conditional image synthesis, and in the process produce (diffusion) features in 2D that we subsequently lift and aggregate on the original surface. 
                Our key observation is that even if the conditional image generations obtained from multi-view rendering of the input shapes are inconsistent, 
                the associated image features are robust and can be directly aggregated across views. 
                This produces semantic features on the input shapes, without requiring additional data or training. 
                We perform extensive experiments on multiple benchmarks (SHREC'19, SHREC'20, and TOSCA) and demonstrate that our features, being semantic instead of geometric, produce reliable correspondence across both isometeric and non-isometrically related shape families.
                <br>
                <!-- <img  src="assets/images/overview.png"> -->
            </p>
        </div>
    </div>
    <div class="white_section_nerf w-container">

        <h2 class="grey-heading_nerf">Results Gallery</h2>
        
        <p>We present dense correspondence results on SHREC'07 [1] to showcase correspondence on shapes other than commonly tested classes of humans and four-legged animals. 
            We show diverse shapes encompassing isometric and highly non-isometric pairs to prove the versatality of our method as a general purpose feature descriptor. 
            Corresponding points are similarly colored.</p>
        <div class="grid-container-4">
    
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <video class="video" loop playsinline autoplay muted src="assets/correspondence/1_source.mp4">
            </div>
    
            <!-- First Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/correspondence/1_target.mp4"></video>
            </div>
    
            <!-- Second Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <video class="video" loop playsinline autoplay muted src="assets/correspondence/2_source.mp4">
            </div>
    
            <!-- Second Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/correspondence/2_target.mp4"></video>
            </div>
    
            <!-- Third Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <video class="video" loop playsinline autoplay muted src="assets/correspondence/3_source.mp4">
            </div>
    
            <!-- Third Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/correspondence/3_target.mp4"></video>
            </div>
    
            <!-- Fourth Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <video class="video" loop playsinline autoplay muted src="assets/correspondence/4_source.mp4">
            </div>
    
            <!-- Fourth Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/correspondence/4_target.mp4"></video>
            </div>

            <!-- First Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <video class="video" loop playsinline autoplay muted src="assets/correspondence/5_source.mp4">
            </div>
    
            <!-- First Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/correspondence/5_target.mp4"></video>
            </div>
    
            <!-- Second Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <video class="video" loop playsinline autoplay muted src="assets/correspondence/6_source.mp4">
            </div>
    
            <!-- Second Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/correspondence/6_target.mp4"></video>
            </div>
        </div>

        <h2 class="grey-heading_nerf">How Does it Work?</h2>
        <p>We aim to decorate 3D points of a given shape in any modality- point clouds or meshes, with rich semantic descriptors. Given the scarcity of 3D geometry data from which to learn these meaningful descriptors, we leverage foundational vision models trained on very large datasets to obtain these features. This enables Diff3F to produce semantic descriptors in a zero-shot way.</p>
        <div class="image-container3">
            <img src="assets/gallery.jpg">
        </div>
        <div class="image-container3">
            <img src="assets/pipeline.jpg">
        </div>
        <div class="container-2 nerf_header_v2 w-container" style="text-align: left;">
        <p> <b>Method overview.</b>
            We render a given shape without textures from multiple views, and the resulting renderings are in-painted by guiding ControlNet with geometric conditions; 
            the generative features from ControlNet are fused with DINO features obtained from the textured rendering, followed by unprojecting to the 3D surface. 
            Note that the textured images obtained by conditioning ControlNet from different views can be inconsistent but when aggregated it produces stable sematic descriptors. Please refer to our paper for more details.</p>
        </div>
        </div>
    </div>


    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf">Semantic Understanding</h2>
        <p>We showcase heatmaps for visualization of our semantic descriptors. For a query point in the source (denoted by a red ball), we see sematically related points being highlighted in the target. The highest similarity point in the target is indicated by a red ball.</p>
        <div class="image-container1">
            <img src="assets/hmap.png">
        </div>
        <div class="grid-container-4">
            <!-- First Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <img src="assets/heatmap_results/hmap10_query.png">
            </div>
    
            <!-- First Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/heatmap_results/hmap10.mp4"></video>
            </div>
    
            <!-- Second Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <img src="assets/heatmap_results/hmap2_query.png">
            </div>
    
            <!-- Second Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/heatmap_results/hmap2.mp4"></video>
            </div>
    
            <!-- Third Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <img src="assets/heatmap_results/hmap9_query.png">
            </div>
    
            <!-- Third Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/heatmap_results/hmap9.mp4"></video>
            </div>
    
            <!-- Fourth Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <img src="assets/heatmap_results/hmap4_query.png">
            </div>
    
            <!-- Fourth Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/heatmap_results/hmap4.mp4"></video>
            </div>

            <!-- First Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <img src="assets/heatmap_results/hmap5_query.png">
            </div>
    
            <!-- First Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/heatmap_results/hmap5.mp4"></video>
            </div>
    
            <!-- <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <img src="assets/heatmap_results/hmap6_query.png">
            </div>
    
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/heatmap_results/hmap6.mp4"></video>
            </div>
    
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <img src="assets/heatmap_results/hmap7_query.png">
            </div>
    
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/heatmap_results/hmap7.mp4"></video>
            </div> -->
    
            <!-- Fourth Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Source</p>
                <img src="assets/heatmap_results/hmap8_query.png">
            </div>
    
            <!-- Fourth Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">Target</p>
                <video class="video" loop playsinline autoplay muted src="assets/heatmap_results/hmap8.mp4"></video>
            </div>
        </div>
    </div>
    <!-- <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf">Evaluation on SHREC'07.</h2>
    <table>
    <caption style="text-align: left;">DIFF3F (ours) attains 3.5x lower error compared to DPC. We were unable to evaluate SE-ORNet on SHREC'07 as the method failed on shape classes where the number of points in the point cloud is low (SHREC'07 has as low as 11 annotated points for some classes).</caption>
    <tr>
      <th><strong>Method</strong></th>
      <th>acc &uarr;</th>
      <th>err &darr;</th>
    </tr>
    <tr>
      <td>DPC [2]</td>
      <td>35.63</td>
      <td>4.91</td>
    </tr>
    <tr>
      <td>SE-ORNet[3]</td>
      <td>✘</td>
      <td>✘</td>
    </tr>
    <tr>
      <td>Diff3F (ours)</td>
      <td><strong>52.73</strong></td>
      <td><strong>1.41</strong></td>
    </tr>
  </table>
    </div> -->

    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf">Zero-Shot Part segmentation</h2>
        <p> K-means clustering can be directly applied to our Diff3F features to extract part segments. Interestingly, we discover that the k-means 
            centroids, extracted from one shape (e.g., human), can be used to segment another (e.g., cat), thanks to the semantic nature of our descriptors. This leads to
            corresponding part segmentation (arms of the human map to front legs of the cat, head maps to head, etc.) as seen in the figure below.</p>
        <div class="grid-container-3">
            <!-- First Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">k=6</p>
                <img src="assets/seg/human_seg_1.png">
            </div>
    
            <!-- First Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">segmented using centroids of human</p>
                <img src="assets/seg/cat_seg_2.png">
            </div>
    
            <!-- Second Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">k=6</p>
                <img src="assets/seg/elephant_seg_4.png">
            </div>
            <!-- Second Video -->
            <div class="grid-item">
                <p class="myprompt nerf_text">k=3</p>
                <img src="assets/seg/chair_seg_3.png">
            </div>
    
            <!-- Third Image -->
            <div class="grid-item">
                <p class="myprompt nerf_text">k=3</p>
                <img src="assets/seg/fish_seg_5.png">
            </div>
    
            <div class="grid-item">
                <p class="myprompt nerf_text">segmented using centroids of dolphin</p>
                <img src="assets/seg/bird_seg_6.png">
            </div>
        </div>
    </div>

    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf">Comparison to Prior Works</h2>
        <br>
        <!-- <div class="grid-container-2">
            <div class="grid-item" style="width: 200px;">
                <img src="assets/compare.jpg">
                <p class="myprompt nerf_text">We compare our DIFF3F (bottom) against SOTA methods (i.e., DPC and SE-ORNet) for the task of point-to-point shape correspondence. 
                    Corresponding points, computed as described in Section 3.4, are similarly colored. We show results using point cloud rendering of our method for the human pair (left) and results with mesh rendering for the animal pair (right). 
        </p>
            </div>
    
            <div class="grid-item" style="width: 300px";>
                <img src="assets/descriptor.jpg">
                <p class="myprompt nerf_text">Source</p>
            </div>
        </div> -->

        <div class="image-container2">
            <img src="assets/visual-comparison.jpg">
        </div>
        <div class="containerside" style="text-align: left;">
        <p><b>Comparisons.</b> We compare our Diff3F (bottom) against SOTA methods (i.e., DPC and SE-ORNet) for the task of point-to-point shape correspondence. 
            Corresponding points are similarly colored. We show results using point cloud rendering of our method for the human pair (left) and results with mesh rendering for the animal pair (right).
            While DPC and SE-ORNet both get confused by the different alignments of the human pair resulting in a laterally flipped prediction, ours, being a multi-view rendering-based method, it is robust to rotation. 
        </p>
        </div>
        <div class="containerside" style="text-align: right;">
            <p><b>Regularizing point-2-point maps.</b> We compare the effectiveness of vanilla functional maps [5] with the Wave Kernel Signature as descriptors [6] (top) vs our descriptors Diff3F (bottom). Ours being semantic enables Functional Maps to work with non-isometric deformations even though FMs typically struggle with such cases when using traditional geometric descriptors. 
                Our descriptors yield accurate correspondence in most cases, thus eliminating the need for further refinement algorithms typically used in related works. </p>
        </div>
    </div>
    <div class="wtable w-container">
        <h2 class="grey-heading_nerf">Evaluation Comparison</h2>
        <table>
            <caption style="text-align: left;"> We report correspondence accuracy within 1% error tolerance, with our method against competing works. The Laplace Beltrami Operator~(LBO) computation for Functional Maps[5] is unstable on TOSCA [7] since the inputs contain non-manifold meshes. 
                By ‘*’ we denote results reported by SE-ORNet [3].</caption>
            <tr>
              <th rowspan="2"><strong>Method &rarr;</strong>
                <br><strong>Dataset &darr; </strong></th>
              <th colspan="2">DPC [2]</th>
              <th colspan="2">SE-ORNet [3]</th>
              <th colspan="2">3DCODED [4]</th>
              <th colspan="2">FM [5]+WKS [6]</th>
              <th colspan="2">Diff3F (ours)</th>
              <th colspan="2">Diff3F (ours)+FM[5]</th>
            </tr>
            <tr>
              <th>acc &uarr;</th>
              <th>err &darr;</th>
              <th>acc &uarr;</th>
              <th>err &darr;</th>
              <th>acc &uarr;</th>
              <th>err &darr;</th>
              <th>acc &uarr;</th>
              <th>err &darr;</th>
              <th>acc &uarr;</th>
              <th>err &darr;</th>
              <th>acc &uarr;</th>
              <th>err &darr;</th>
            </tr>
            <tr>
              <td>TOSCA [7]</td>
              <td><u><i>30.79</i></u></td>
              <td><strong>3.74</strong></td>
              <td><strong>33.25</strong></td>
              <td><u><i>4.32</i></u></td>
              <td>0.5*</td>
              <td>19.2*</td>
              <td colspan="2">✘</td>
              <td>20.27</td>
              <td>5.69</td>
              <td colspan="2">✘</td>
            </tr>
            <tr>
              <td>SHREC'19 [8]</td>
              <td>17.40</td>
              <td>6.26</td>
              <td>21.41</td>
              <td>4.56</td>
              <td>2.10</td>
              <td>8.10</td>
              <td>4.37</td>
              <td>3.26</td>
              <td><strong>26.41</strong></td>
              <td><u><i>1.69</i></u></td>
              <td><u><i>21.55</i></u></td>
              <td><strong>1.49</strong></td>
            </tr>
            <tr>
              <td>SHREC'20 [9]</td>
              <td>31.08</td>
              <td>2.13</td>
              <td>31.70</td>
              <td>1.00</td>
              <td colspan="2">✘</td>
              <td>4.13</td>
              <td>7.29</td>
              <td><strong>72.60</strong></td>
              <td><u><i>0.93</i></u></td>
              <td><u><i>62.34</i></u></td>
              <td><strong>0.71</strong></td>
            </tr>
            </table>
        
    </div>
    <div class="wtable w-container">
        <h2 class="grey-heading_nerf">Generalization</h2>
        <table>
            <caption style="text-align: left;"> We compare generalization capabilities of Diff3F vs others, by training on one dataset and testing on a different set. 
                For DPC and SE-ORNet, we choose SURREAL and SMAL as the training sets for human and animal shapes respectively as these datasets are significantly larger and lead to improved generalization scores. 
                By '*' we denote results reported by SE-ORNet [3]. Ours sees significantly higher scores for generalization.</caption>
            <tr>
              <th rowspan="2"><strong>Train</strong></th>
              <th rowspan="2"><strong>Method</strong></th>
              <th colspan="2">TOSCA [7]</th>
              <th colspan="2">SHREC'19 [8]</th>
              <th colspan="2">SHREC'20 [9]</th>
            </tr>
            <tr>
              <th>acc &uarr;</th>
              <th>err &darr;</th>
              <th>acc &uarr;</th>
              <th>err &darr;</th>
              <th>acc &uarr;</th>
              <th>err &darr;</th>
            </tr>
            <tr>
              <td rowspan="2">SURREAL</td>
              <td>DPC [2]</td>
              <td>29.30</td>
              <td><u><i>5.25</i></u></td>
              <td>17.40</td>
              <td>6.26</td>
              <td>31.08</td>
              <td>2.13</td>
            </tr>
            <tr>
              <td>SE-ORNET [3]</td>
              <td>16.71</td>
              <td>9.19</td>
              <td><u><i>21.41</i></u></td>
              <td><u><i>4.56</i></u></td>
              <td><u><i>31.70</i></u></td>
              <td><u><i>1.00</i></u></td>
            </tr>
            <tr>
              <td rowspan="2">SMAL</td>
              <td>DPC [2]</td>
              <td><u><i>30.28</i></u></td>
              <td>6.43</td>
              <td>12.34</td>
              <td>8.01</td>
              <td>24.5*</td>
              <td>7.5*</td>
            </tr>
            <tr>
              <td>SE-ORNET [3]</td>
              <td><strong>31.59</strong></td>
              <td><strong>4.76</strong></td>
              <td>12.49</td>
              <td>9.87</td>
              <td>25.4*</td>
              <td>2.9*</td>
            </tr>
            <tr>
              <td>Pretrained</td>
              <td>Diff3F</td>
              <td>20.27</td>
              <td>5.69</td>
              <td><strong>26.41</strong></td>
              <td><strong>1.69</strong></td>
              <td><strong>72.60</strong></td>
              <td><strong>0.93</strong></td>
            </tr>
          </table>
    </div>

    <br><br>
    

    

<div class="white_section_nerf_ref w-container">
<h2 class="grey-heading_nerf">References</h2>
    <br>
    [1] Daniela Giorgi, Silvia Biasotti, and Laura Paraboschi. Shape077
    retrieval contest 2007: Watertight models track. SHREC competition, 8(7):7, 2007.<br>
    [2] Itai Lang, Dvir Ginzburg, Shai Avidan, and Dan Raviv. Dpc: Unsupervised deep point correspondence via cross and self construction. In 2021 International Conference on 3D Vision (3DV), pages 1442–1451. IEEE, 2021. 
    <br>
    [3] Jiacheng Deng, Chuxin Wang, Jiahao Lu, Jianfeng He, Tianzhu Zhang, Jiyang Yu, and Zhe Zhang. Se-ornet: Self- ensembling orientation-aware network for unsupervised point cloud shape correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5364–5373, 2023.
    <br>
    [4] Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, and Mathieu Aubry. 3d-coded: 3d correspondences by deep deformation. In Proceedings of the european conference on computer vision (ECCV), pages 230–246, 2018.
    <br>
    [5] Maks Ovsjanikov, Mirela Ben-Chen, Justin Solomon, Adrian Butscher, and Leonidas Guibas. Functional maps: a flexible representation of maps between shapes. ACM Transactions on Graphics (ToG), 31(4):1–11, 2012.
    <br>
    [6] Mathieu Aubry, Ulrich Schlickewei, and Daniel Cremers. The wave kernel signature: A quantum mechanical approach to shape analysis. In 2011 IEEE international conference on computer vision workshops (ICCV workshops), pages 1626– 1633. IEEE, 2011. 
    <br>
    [7] Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel. Numerical geometry of non-rigid shapes. Springer Science & Business Media, 2008.
    <br>
    [8] Simone Melzi, Riccardo Marin, Emanuele Rodola`, Umberto Castellani, Jing Ren, Adrien Poulenard, Peter Wonka, and Maks Ovsjanikov. Shrec 2019: Matching humans with different connectivity. In Eurographics Workshop on 3D Object Retrieval, page 3. The Eurographics Association, 2019.
    <br> 
    [9] Roberto M Dyke, Yu-Kun Lai, Paul L Rosin, Stefano Zappala`, Seana Dykes, Daoliang Guo, Kun Li, Riccardo Marin, Simone Melzi, and Jingyu Yang. Shrec’20: Shape correspondence with non-isometric deformations. Computers & Graphics, 92:28–43, 2020.
    <br><br><br>
    <!-- <pre><code>

  </code></pre> -->
</div>


<div class="white_section_nerf grey_container w-container">
    <h2 class="grey-heading_nerf">BibTeX</h2>
    <div class="bibtex">
        <pre><code>
@article{dutt2023diffusion,
    title={Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with Distilled Semantic Features}, 
    author={Dutt, Niladri Shekhar and Muralikrishnan, Sanjeev and Mitra, Niloy J.},
    journal={arXiv preprint arXiv:2311.17024},
    year={2023},
}            
      </code></pre>
    </div>
    </div>

</body>
<footer>
    Template adapted from <a href="https://mrtornado24.github.io/DreamCraft3D/" target="_blank">DreamCraft3D</a> <br>
    <!-- <div style="width: 60px; height: 20px; display: flex; align-items: center; justify-content: center;">
        <a href="https://hits.seeyoufarm.com">
            <img style="width: 100%;" src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fdiff3f.github.io&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false" alt="hits"/>
        </a>
    </div> -->
    
    <br><br><br>
</footer> 
</html>
